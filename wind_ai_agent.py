import asyncio
import json
import os
from datetime import datetime
from typing import List, Optional
from pathlib import Path
from crawl4ai import BrowserConfig, CrawlerRunConfig
from urllib.parse import urlparse, urlunparse
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy
import re
# æ ¸å¿ƒç»„ä»¶
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig, CacheMode, LLMConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator

# å¯¼å…¥ä½ çš„ Schema
from wind_schema import WindExtraction, WindArticle


class WindDeepAgent:
    # def __init__(self, ollama_model: str = "ollama/qwen3-coder:480b-cloud"):
    #     self.output_dir = Path("wind_intelligence_data")
    #     self.output_dir.mkdir(parents=True, exist_ok=True)
    #
    #     # é…ç½® æœ¬åœ°ollama LLM
    #     self.llm_config = LLMConfig(provider=ollama_model)
    #
    #     # å¹¶å‘æ§åˆ¶ï¼šåŒæ—¶å¤„ç† 1 ä¸ªè¯¦æƒ…é¡µï¼Œé¿å… Ollama/API è¿‡è½½
    #     self.semaphore = asyncio.Semaphore(1)
    def __init__(self):
        self.output_dir = Path("wind_intelligence_data")
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # 1. è®¾ç½® API åœ°å€ (DeepSeek å®˜æ–¹åœ°å€)
        os.environ["OPENAI_API_BASE"] = "https://api.deepseek.com"

        # 2. è®¾ç½® API Key (å¿…é¡»æ›¿æ¢ä¸ºä½ è‡ªå·±çš„ sk-xxxx)
        os.environ["OPENAI_API_KEY"] = ""

        # 3. åˆå§‹åŒ–é…ç½®
        self.llm_config = LLMConfig(
            provider="openai/deepseek-chat",
        )

        # 4. è¿™é‡Œçš„å¹¶å‘å¯ä»¥å¼€å¤§ï¼Œäº‘ç«¯å¤„ç†å¾ˆå¿«
        self.semaphore = asyncio.Semaphore(5)

    async def run(self, start_url: str):
        """ä¸»æµç¨‹ï¼šåˆ—è¡¨æŠ“å– -> è¯¦æƒ…æŠ“å– -> æŠ¥å‘Šç”Ÿæˆ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        run_folder = self.output_dir / f"task_{timestamp}"
        run_folder.mkdir(exist_ok=True)

        # --- æ­¥éª¤ 1: æŠ“å–åˆ—è¡¨é¡µ ---
        print(f"ğŸ”¥ [Stage 1] æ­£åœ¨æ‰«æåˆ—è¡¨é¡µ: {start_url}")
        extraction_data = await self._crawl_listing_page(start_url)

        if not extraction_data or not extraction_data.news_articles:
            print("âŒ æœªå‘ç°æ–‡ç« é“¾æ¥ï¼Œä»»åŠ¡ç»ˆæ­¢ã€‚")
            return

        print(f"âœ… å‘ç° {len(extraction_data.news_articles)} ç¯‡æ–‡ç« ï¼Œå‡†å¤‡è¿›è¡Œæ·±åº¦åˆ†æ...")


        # --- æ­¥éª¤ 2: å¹¶å‘æŠ“å–è¯¦æƒ…é¡µ ---

        target_articles = extraction_data.news_articles[:5]#æŠŠ [:5] å»æ‰å°±å¯ä»¥è·‘å…¨é‡

        tasks = []
        for article in target_articles:
            tasks.append(self._process_detail_page(article, run_folder))

        # ç­‰å¾…æ‰€æœ‰è¯¦æƒ…é¡µä»»åŠ¡å®Œæˆ
        detailed_articles = await asyncio.gather(*tasks)

        # è¿‡æ»¤æ‰æŠ“å–å¤±è´¥çš„ (None)
        valid_articles = [art for art in detailed_articles if art is not None]

        # æ›´æ–°ä¸»æ•°æ®å¯¹è±¡
        extraction_data.news_articles = valid_articles

        # --- æ­¥éª¤ 3: ä¿å­˜ä¸æŠ¥å‘Š ---
        self._save_final_data(extraction_data, run_folder)
        self._generate_markdown_report(extraction_data, run_folder)

    async def _crawl_listing_page(self, url: str) -> Optional[WindExtraction]:
        """é˜¶æ®µä¸€ï¼šé€šç”¨å¯å‘å¼æå–å¼•æ“ (æ— ç‰¹å®šç½‘ç«™ä¾èµ–)"""
        from urllib.parse import urljoin, urlparse
        import re

        print(f"âš¡ [Stage 1] æ­£åœ¨é€šè¿‡é€šç”¨å¼•æ“åˆ†æåˆ—è¡¨é¡µ: {url}")

        # 1. åŸºç¡€é…ç½® (ä»…ä½¿ç”¨æœ€é€šç”¨çš„å‚æ•°)
        browser_conf = BrowserConfig(headless=True)
        config = CrawlerRunConfig(
            cache_mode=CacheMode.BYPASS,
            page_timeout=60000,
            wait_until="domcontentloaded",
        )

        async with AsyncWebCrawler(config=browser_conf) as crawler:
            try:
                result = await crawler.arun(url=url, config=config)
                if not result.success: return None
            except Exception as e:
                print(f"âŒ è¿æ¥å¼‚å¸¸: {e}");
                return None

            unique_articles = {}
            # åŠ¨æ€è·å–å½“å‰ç«™ç‚¹çš„ä¸»åŸŸåï¼Œä¸å†å†™æ­»
            parsed_start_url = urlparse(url)
            current_host = parsed_start_url.netloc
            # è·å–æ ¹åŸŸå (ä¾‹å¦‚: fd.bjx.com.cn -> bjx.com.cn)
            domain_parts = current_host.split('.')
            root_domain = ".".join(domain_parts[-2:]) if len(domain_parts) >= 2 else current_host

            # 2. å¢å¼ºæå–é€»è¾‘ (é€šç”¨ç‰¹å¾)
            # A. æ’é™¤å…³é”®è¯ï¼šä¸ç®¡æ˜¯å“ªä¸ªç«™ï¼Œè¿™äº›é€šå¸¸éƒ½ä¸æ˜¯æ–°é—»
            exclude_pattern = re.compile(
                r'(about|contact|join|login|register|copyright|help|search|feedback|service|career|privacy|member|legal|apply|å…³äº|è”ç³»|æ‹›è˜|å£°æ˜|ç™»å½•|æ³¨å†Œ|ä¸‹è½½)',
                re.I)

            # B. éå†æ‰€æœ‰é“¾æ¥ (Crawl4AI å·²ç»è§£æå¥½äº†åŸºç¡€æ•°æ®)
            all_links = result.links.get("internal", []) + result.links.get("external", [])
            print(f"ğŸ” å‘ç°åŸå§‹é“¾æ¥: {len(all_links)} ä¸ª")

            for link in all_links:
                href = link.get('href', '')
                title = link.get('text', '').strip()
                full_url = urljoin(url, href)
                parsed_link = urlparse(full_url)

                # --- è¿‡æ»¤å™¨ 1: åŸŸåå®‰å…¨æ£€æŸ¥ ---
                if root_domain not in parsed_link.netloc:
                    continue

                # --- è¿‡æ»¤å™¨ 2: æ’é™¤éæ–°é—»é¡µé¢ ---
                if exclude_pattern.search(full_url) or exclude_pattern.search(title):
                    continue

                # --- è¿‡æ»¤å™¨ 3: è¯¦æƒ…é¡µç‰¹å¾è¯„åˆ† (æ ¸å¿ƒé€šç”¨é€»è¾‘) ---
                path = parsed_link.path.lower()

                # ç‰¹å¾ A: URL åŒ…å« 4 ä½åŠä»¥ä¸Šè¿ç»­æ•°å­— (è¿™æ˜¯æ–°é—» ID æˆ–æ—¥æœŸçš„é€šç”¨æ ‡å¿—)
                has_id_feature = len(re.findall(r'\d{4,}', path)) > 0

                # ç‰¹å¾ B: å¸¸è§æ–°é—»æ–‡ç« åç¼€
                is_article_ext = path.endswith(('.html', '.shtml', '.htm', '.php', '.jsp')) or path == ""

                # ç‰¹å¾ C: è·¯å¾„æ·±åº¦ã€‚è¯¦æƒ…é¡µé€šå¸¸åœ¨ /news/2024/01.html (æ·±åº¦ >= 2)
                # åˆ—è¡¨é¡µé€šå¸¸æ˜¯ /news/ (æ·±åº¦ = 1)
                path_depth = len([p for p in path.split('/') if p])

                # --- æ ‡é¢˜è¡¥å…¨é€»è¾‘ ---
                # å¦‚æœæ–‡å­—ä¸ºç©ºï¼Œå°è¯•ä» result.html ä¸­æš´åŠ›æå–è¯¥é“¾æ¥å¯¹åº”çš„ title å±æ€§
                # è¿™åœ¨å¾ˆå¤šè€æ—§æˆ–å›¾ç‰‡è¾ƒå¤šçš„ç½‘ç«™ä¸­éå¸¸ç®¡ç”¨
                if len(title) < 5:
                    # å°è¯•æ­£åˆ™ä»æºç ä¸­æ‰¾è¯¥ href å¯¹åº”çš„ title å±æ€§
                    # <a ... href="xxx" ... title="è¿™æ˜¯æ ‡é¢˜" ...>
                    attr_match = re.search(fr'href=["\']{re.escape(href)}["\'][^>]*title=["\']([^"\']+)["\']',
                                           result.html, re.I)
                    if attr_match:
                        title = attr_match.group(1).strip()

                # --- ç»¼åˆåˆ¤å®š ---
                # è§„åˆ™ï¼š(æœ‰æ•°å­— ID ä¸” æ˜¯é™æ€é¡µ) æˆ–è€… (è·¯å¾„è¶³å¤Ÿæ·± ä¸” æ ‡é¢˜é•¿)
                if (has_id_feature and is_article_ext) or (path_depth >= 2 and len(title) > 12):
                    if full_url not in unique_articles:
                        unique_articles[full_url] = WindArticle(
                            title=title if title else "æœªæ•è·æ ‡é¢˜",
                            url=full_url,
                            category="å…¶ä»–"
                        )

            # 3. ç»“æœå…œåº•ï¼šå¦‚æœä¸€æ¡éƒ½æ²¡æŠ“åˆ°ï¼Œé‡‡ç”¨â€œé“¾æ¥æ¨¡å¼ç»Ÿè®¡æ³•â€
            # æœ‰äº›ç½‘ç«™å°±æ˜¯æ²¡åç¼€æ²¡ IDï¼Œæ­¤æ—¶æˆ‘ä»¬æŠ“å–æ‰€æœ‰åŒ…å«æœ¬ç«™åŸŸåä¸”æ ‡é¢˜å¤Ÿé•¿çš„é“¾æ¥
            if not unique_articles:
                print("âš ï¸ å¯å‘å¼è¯„åˆ†æœªå‘½ä¸­ï¼Œå°è¯•åŸºäºæ ‡é¢˜é•¿åº¦çš„é€šç”¨æ‰«æ...")
                for link in all_links:
                    full_url = urljoin(url, link.get('href', ''))
                    title = link.get('text', '').strip()
                    if root_domain in full_url and len(title) > 15:
                        if full_url not in unique_articles:
                            unique_articles[full_url] = WindArticle(title=title, url=full_url, category="å…¶ä»–")

            print(f"ğŸ“Š é€šç”¨è¯†åˆ«å®Œæˆ: æ‰¾åˆ° {len(unique_articles)} æ¡æ½œåœ¨æ–°é—»é“¾æ¥")

            # å°†å­—å…¸è½¬å›åˆ—è¡¨
            data_obj = WindExtraction(
                source_url=url,
                website_title="è¡Œä¸šæ·±åº¦æƒ…æŠ¥åˆ†æ",
                news_articles=list(unique_articles.values()),
                wind_keywords=["WindPower", "Energy_Intelligence"]
            )
            return data_obj

    async def _process_detail_page(self, simple_article: WindArticle, folder: Path) -> Optional[WindArticle]:
        """é˜¶æ®µäºŒï¼šè¯¦æƒ…é¡µæå–"""

        # å¿…é¡»å¯¼å…¥æ­£åˆ™æ¨¡å—
        import re

        async with self.semaphore:
            print(f"   â¬‡ï¸ è¿›å…¥è¯¦æƒ…: {simple_article.title[:15]}...")

            # 1. æµè§ˆå™¨ä¼ªè£… (User-Agent)
            browser_conf = BrowserConfig(
                headless=True,
                headers={
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36",
                }
            )

            # 2. æç¤ºè¯ï¼šå¼ºè°ƒæ—¥æœŸå’Œæ‘˜è¦
            strategy = LLMExtractionStrategy(
                llm_config=self.llm_config,
                schema=WindArticle.model_json_schema(),
                instruction="""
                ä½ æ˜¯ä¸€ä½èµ„æ·±çš„é£ç”µè¡Œä¸šæƒ…æŠ¥åˆ†æå¸ˆã€‚è¯·ä»ç½‘é¡µå†…å®¹æå–å…³é”®ä¿¡æ¯ã€‚
                
                ### 1. æ ¸å¿ƒä»»åŠ¡ï¼šæ–‡ç« åˆ†ç±» (category)
                è¯·æ ¹æ®æ–‡ç« çš„æ ¸å¿ƒä¸»æ—¨ï¼Œå°† `category` å­—æ®µä¸¥æ ¼è®¾å®šä¸ºä»¥ä¸‹ä¸‰è€…ä¹‹ä¸€ï¼š
                - **"æŠ€æœ¯"**: æ¶‰åŠé£æœºåˆ¶é€ ã€å¶ç‰‡ææ–™ã€è¿ç»´æŠ€æœ¯ã€æ–½å·¥å·¥è‰ºã€æ–°äº§å“å‘å¸ƒã€æŠ€æœ¯å‚æ•°ã€ä¸“åˆ©ã€ç§‘ç ”æˆæœç­‰ã€‚
                - **"æ”¿ç­–"**: æ¶‰åŠå›½å®¶/åœ°æ–¹å‘å¸ƒçš„é€šçŸ¥ã€åå››äº”è§„åˆ’ã€ç”µä»·è¡¥è´´ã€ç®¡ç†åŠæ³•ã€è¡Œä¸šæ ‡å‡†ã€æ ¸å‡†æ‰¹å¤ã€ç«é…è§„åˆ™ç­‰ã€‚
                - **"å¸‚åœº"**: æ¶‰åŠä¸­æ ‡ç»“æœã€ä¼ä¸šåŠ¨æ€ã€æŠ•èèµ„ã€è£…æœºæ•°æ®ç»Ÿè®¡ç­‰ï¼ˆå¦‚æœä¸å±äºæŠ€æœ¯æˆ–æ”¿ç­–ï¼Œå½’ä¸ºæ­¤ç±»ï¼‰ã€‚

                ### 2. å…¶ä»–æå–ä»»åŠ¡
                - **æ—¥æœŸ (publish_date)**: å¯»æ‰¾å‘å¸ƒæ—¶é—´ï¼Œæ ¼å¼å¿…é¡»ä¸º YYYY-MM-DDã€‚
                - **æ‘˜è¦ (summary)**: æ€»ç»“æ ¸å¿ƒäº‹å®ï¼ˆæ¶‰åŠçš„é‡‘é¢ã€å…·ä½“å‚æ•°ã€ç›¸å…³å…¬å¸ï¼‰ï¼Œ150å­—ä»¥å†…ï¼Œå®¢è§‚é™ˆè¿°ã€‚
                - **æ ‡ç­¾ (tags)**: æå– 3-5 ä¸ªå…·ä½“çš„å®ä½“å…³é”®è¯ã€‚
                """
            )

            # 3. è¿è¡Œé…ç½®
            config = CrawlerRunConfig(
                extraction_strategy=strategy,
                cache_mode=CacheMode.BYPASS,

                # è¿™é‡Œç»å¯¹ä¸èƒ½æ”¾ CSS ç±»å(å¦‚ .ads)ï¼Œåªèƒ½æ”¾æ ‡ç­¾åï¼
                excluded_tags=["nav", "footer", "script", "style", "noscript", "aside"],

                # è‡ªåŠ¨ç§»é™¤é®ç½©å±‚
                remove_overlay_elements=True,

                # ç­‰å¾…é¡µé¢åŠ è½½
                delay_before_return_html=3.0,
            )

            async with AsyncWebCrawler(config=browser_conf) as crawler:
                try:
                    result = await crawler.arun(url=simple_article.url, config=config)
                except Exception as e:
                    print(f"   âŒ æµè§ˆå™¨å´©æºƒ: {e}")
                    return None

                # æ£€æŸ¥æ˜¯å¦æ˜¯ Crawl4AI çš„é”™è¯¯é¡µé¢
                if not result.success or (result.markdown and "Crawl4AI Error" in result.markdown):
                    print(f"   âŒ æŠ“å–è¢«æ‹¦æˆªæˆ–æŠ¥é”™: {simple_article.url}")
                    return None

                if result.extracted_content:
                    try:
                        raw = json.loads(result.extracted_content)
                        if not raw:
                            data_dict = {}
                        else:
                            data_dict = raw[0] if isinstance(raw, list) else raw

                        # åŸºç¡€æ•°æ®å›å¡«
                        if not data_dict.get('title'): data_dict['title'] = simple_article.title
                        data_dict['url'] = simple_article.url

                        # æ‘˜è¦å…œåº•ï¼šå¦‚æœ LLM æ²¡ç”Ÿæˆï¼Œæ‰‹åŠ¨æˆªå–å‰200å­—
                        if not data_dict.get('summary'):
                            clean_md = result.markdown[:300].replace('\n', ' ') if result.markdown else ""
                            data_dict['summary'] = f"{clean_md}..."

                        ai_date = data_dict.get('publish_date')

                        # å¦‚æœ AI æ²¡å¡«æ—¥æœŸï¼Œæˆ–è€…æ—¥æœŸçœ‹èµ·æ¥ä¸å¯¹ï¼Œæˆ‘ä»¬è‡ªå·±æœ
                        if not ai_date or len(str(ai_date)) < 8:
                            full_text = result.markdown or result.cleaned_html or ""

                            # æ­£åˆ™åŒ¹é…ï¼š2026-01-09 æˆ– 2026å¹´1æœˆ9æ—¥
                            match = re.search(r'(\d{4})[å¹´/-](\d{1,2})[æœˆ/-](\d{1,2})', full_text)

                            if match:
                                y, m, d = match.groups()
                                found_date = f"{y}-{int(m):02d}-{int(d):02d}"
                                data_dict['publish_date'] = found_date
                            else:
                                # æœ€åçš„å°è¯•ï¼šåœ¨ URL é‡Œæ‰¾æ—¥æœŸ (æœ‰äº› URL åŒ…å«æ—¥æœŸ)
                                url_match = re.search(r'202\d{5}', simple_article.url)
                                if url_match:
                                    d_str = url_match.group()
                                    data_dict['publish_date'] = f"{d_str[:4]}-{d_str[4:6]}-{d_str[6:]}"

                        # ===============================================

                        rich_article = WindArticle(**data_dict)
                        # æ‰“å°æ—¥å¿—ï¼šæ ‡é¢˜ + æ—¥æœŸ
                        print(f"   âœ… åˆ†ææˆåŠŸ: {rich_article.title[:10]}... | ğŸ“… {rich_article.publish_date}")
                        return rich_article

                    except Exception as e:
                        print(f"   âŒ æ•°æ®å¤„ç†å¼‚å¸¸: {e}")
                        return None
                else:
                    print(f"   âš ï¸ æ— å†…å®¹è¿”å›")
                    return None

    def _save_final_data(self, data: WindExtraction, folder: Path):
        """ä¿å­˜å®Œæ•´çš„ JSON æ•°æ®"""
        file_path = folder / "full_data.json"
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(data.model_dump(), f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ ç»“æ„åŒ–æ•°æ®å·²ä¿å­˜: {file_path}")

    def _generate_markdown_report(self, data: WindExtraction, folder: Path):
        """ç”Ÿæˆ Markdown æŠ¥å‘Š (æŒ‰ç±»åˆ«åˆ†ç»„å±•ç¤º)"""
        report_path = folder / "Wind_Analysis_Report.md"

        md = f"""# ğŸŒ¬ï¸ {data.website_title} - æ·±åº¦åˆ†ææŠ¥å‘Š

**æ¥æº**: {data.source_url}  
**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M')}  
**å…¨ç«™å…³é”®è¯**: {', '.join(data.wind_keywords)}

---
"""

        grouped_articles = {
            "æ”¿ç­–": [],
            "æŠ€æœ¯": [],
            "å¸‚åœº": [],
            "å…¶ä»–": []
        }

        for art in data.news_articles:
            # å®¹é”™å¤„ç†ï¼šå¦‚æœ LLM è¾“å‡ºçš„ä¸æ˜¯æ ‡å‡†è¯ï¼Œåšä¸€ä¸ªç®€å•çš„æ˜ å°„
            cat = art.category
            if "æ”¿ç­–" in cat or "Policy" in cat:
                grouped_articles["æ”¿ç­–"].append(art)
            elif "æŠ€æœ¯" in cat or "Tech" in cat:
                grouped_articles["æŠ€æœ¯"].append(art)
            elif "å¸‚åœº" in cat or "Market" in cat:
                grouped_articles["å¸‚åœº"].append(art)
            else:
                grouped_articles["å…¶ä»–"].append(art)

        # === éå†åˆ†ç»„ç”ŸæˆæŠ¥å‘Š ===
        # å®šä¹‰æ˜¾ç¤ºé¡ºåºå’Œå›¾æ ‡
        categories_order = [
            ("ğŸ“œ æ”¿ç­–æ³•è§„", grouped_articles["æ”¿ç­–"]),
            ("âš™ï¸ å‰æ²¿æŠ€æœ¯", grouped_articles["æŠ€æœ¯"]),
            ("ğŸ“ˆ å¸‚åœºåŠ¨æ€", grouped_articles["å¸‚åœº"]),
            ("ğŸ”— å…¶ä»–èµ„è®¯", grouped_articles["å…¶ä»–"])
        ]

        for title, articles in categories_order:
            if not articles:
                continue

            md += f"\n## {title} (å…± {len(articles)} ç¯‡)\n\n"

            for idx, art in enumerate(articles, 1):
                flags = []
                if art.has_project_info: flags.append("ğŸ—ï¸ é¡¹ç›®")
                if art.has_technical_specs: flags.append("ğŸ“ å‚æ•°")
                flag_str = f"| **ç‰¹å¾**: {' '.join(flags)}" if flags else ""

                md += f"### {idx}. {art.title}\n"
                md += f"- **æ—¥æœŸ**: {art.publish_date} | **æ ‡ç­¾**: `{', '.join(art.tags)}` {flag_str}\n"

                # å¼•ç”¨å—æ‘˜è¦
                summary_text = art.summary.replace('\n', '\n> ') if art.summary else "æš‚æ— æ‘˜è¦"
                md += f"> {summary_text}\n\n"
                md += f"[ğŸ”— é˜…è¯»åŸæ–‡]({art.url})\n\n"

            md += "---\n"

        with open(report_path, "w", encoding="utf-8") as f:
            f.write(md)
        print(f"ğŸ“ åˆ†ç±»æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")


async def main():
    # æ›¿æ¢ç›®æ ‡ç½‘ç«™
    target_url = "https://www.woodmac.com/events/global/"
    # https://fd.bjx.com.cn/
    # https://www.in-en.com/
    # https://wind.imarine.cn/offshorewind
    # https://cleantechnica.com/
    # https://www.china5e.com/new-energy/wind-energy/
    # http://www.eastwp.net/news/
    # https://www.woodmac.com/events/global/
    agent = WindDeepAgent()
    await agent.run(target_url)


if __name__ == "__main__":
    asyncio.run(main())